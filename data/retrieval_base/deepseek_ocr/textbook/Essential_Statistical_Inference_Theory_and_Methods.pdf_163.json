[
  {
    "page_idx": 0,
    "text": "Table 3.3 \\( P \\)-values for three samples, \\( n_1 = n_2 = n_3 = 5 \\), \\( \\sigma^2 = 14 \\)\n\n\\[\\begin{array}{cccccc}\n\\overline{Y}_1 & \\overline{Y}_2 & \\overline{Y}_3 & \\text{ANOVA} & \\text{ISO} & \\text{Reg} \\\\\n2 & 6 & 6 & 0.149 & 0.050 & 0.045 \\\\\n2 & 6 & 7 & 0.082 & 0.026 & 0.017 \\\\\n2 & 6 & 8 & 0.036 & 0.011 & 0.006 \\\\\n\\end{array}\\]\n\nrelative ease of using regression is probably why isotonic regression is not used more.\n\nFinally, although the \\( p \\)-values reported in Table 3.3 are for the known-variance case, qualitatively similar results are obtained for the more complicated case of \\( \\sigma \\) unknown.\n\nThere is a large literature on order-restricted inference. For testing for ordered alternatives, there has been more emphasis on \\( T_{LR} \\) than on \\( T_W \\) and \\( T_S \\). The classic references are Barlow et al. (1972) and Robertson et al. (1988), whereas a more recent account is Silvapulle and Sen (2005).\n\n3.6.2 Null Hypotheses on the Boundary of the Parameter Space\n\nWhen a null hypothesis value, say \\( \\theta_0 \\) lies on the boundary of the parameter space, then maximum likelihood estimators are often truncated at that boundary because by definition \\( \\hat{\\theta}_{MLE} \\) must lie in the parameter space of \\( \\theta \\). Thus \\( \\hat{\\theta}_{MLE} \\) is equal to the boundary value \\( \\theta_0 \\) with positive probability and correspondingly \\( T_{LR} \\) is zero for those cases. The result is that the limiting distribution of \\( T_{LR} \\) is a mixture of a point mass at zero and a chi-squared distribution. We illustrate first with an artificial example and then consider the one-way random effects model.\n\n3.6.2a Normal Mean with Restricted Parameter Space\n\nSuppose that \\( Y_1, \\ldots, Y_n \\sim N(\\mu, 1) \\). Usually, \\( \\hat{\\mu}_{MLE} = \\overline{Y} \\), but suppose that we restrict the parameter space for \\( \\mu \\) to be \\( [\\mu_0, \\infty) \\) where \\( \\mu_0 \\) is some given constant, instead of \\( (-\\infty, \\infty) \\). Then \\( \\hat{\\mu}_{MLE} = \\overline{Y} \\) if \\( \\overline{Y} \\geq \\mu_0 \\) and \\( \\hat{\\mu}_{MLE} = \\mu_0 \\) if \\( \\overline{Y} < \\mu_0 \\). Now suppose that the null hypothesis is \\( H_0 : \\mu = \\mu_0 \\). We first consider the three likelihood-based test statistics, showing that only the score statistic has a limiting \\( \\chi^2_1 \\) distribution. Then we provide a simple solution to this testing problem.\n\nUnder \\( H_0 \\), the Wald statistic is \\( T_W = n(\\hat{\\mu}_{MLE} - \\mu_0)^2 \\), which is thus \\( T_W = 0 \\) if \\( \\hat{\\mu}_{MLE} = \\mu_0 \\) and \\( T_W = n(\\overline{Y} - \\mu_0)^2 \\) if \\( \\overline{Y} \\geq \\mu_0 \\). The score statistic is \\( T_S = n(\\overline{Y} - \\mu_0)^2 \\), and the likelihood ratio statistic is the same as the Wald statistic. Thus, only the score statistic converges to a \\( \\chi^2_1 \\) distribution under \\( H_0 \\). The Wald and the likelihood ratio statistics converge to a distribution that is an equal mixture of a point mass at 0 and a \\( \\chi^2_1 \\) distribution, the same distribution as in (3.23) for \\( k = 2 \\). In fact the"
  }
]